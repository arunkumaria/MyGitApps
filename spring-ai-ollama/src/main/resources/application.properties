spring.application.name=spring-ai-ollama
# Point Spring AI to your local Ollama instance (default port)
spring.ai.ollama.base-url=http://localhost:11434

# Specify the model you want to use (e.g., llama3.1, mistral, gemma)
# Make sure you have pulled this model using 'ollama run <model-name>'
spring.ai.ollama.chat.options.model=llama3.1